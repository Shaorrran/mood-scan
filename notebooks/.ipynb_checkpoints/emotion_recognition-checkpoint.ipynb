{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odWL42FWV_y7"
   },
   "source": [
    "# Emotion recognition model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmNmT-YjWD74"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5ZobdR2UcxJ",
    "outputId": "0378338d-3d8f-45cb-ccc8-7611eba9979e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: torchinfo in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from gdown) (4.10.0)\n",
      "Requirement already satisfied: six in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from gdown) (3.4.2)\n",
      "Requirement already satisfied: requests[socks] in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from gdown) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from gdown) (4.52.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from requests[socks]->gdown) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/shaorrran/.virtualenvs/dls/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown torchinfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3SROwg-dU0J9"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import contextlib\n",
    "import functools\n",
    "import glob\n",
    "import IPython\n",
    "import itertools\n",
    "import pathlib\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qsz6en6JD9f"
   },
   "source": [
    "## Setting random states for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E9_2mLPOJHda"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYCZ-v5CWFm0"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EA58UsN2WHKN"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CONTEXT_IMAGE_SIZE = 224\n",
    "BODY_IMAGE_SIZE = 128\n",
    "CONTEXT_NORM = {\"mean\": [0.4690646, 0.4407227, 0.40508908], \"std\": [0.2514227, 0.24312855, 0.24266963]}\n",
    "BODY_NORM = {\"mean\": [0.43832874, 0.3964344, 0.3706214], \"std\": [0.24784276, 0.23621225, 0.2323653]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru86tqojYR-n"
   },
   "source": [
    "## Annotations loading and converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeUQ-QNjYUYc",
    "outputId": "1666c232-cf9b-4bc0-ac0a-a39bc3c99a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HPkjTRC1I9dNSrdarzdybdIM7cKIJPMg\n",
      "To: /home/shaorrran/Projects/ITMO/Hackathon/ICT.HACK-IV/notebooks/Annotations.zip\n",
      "100%|██████████████████████████████████████| 6.40M/6.40M [00:02<00:00, 2.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1HPkjTRC1I9dNSrdarzdybdIM7cKIJPMg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nc8cghObYi8R"
   },
   "outputs": [],
   "source": [
    "def prepare_annotations(filename):\n",
    "    if not pathlib.Path(\"annotations.csv\").is_file():\n",
    "        if \"google.colab\" in str(IPython.get_ipython()):\n",
    "            with zipfile.ZipFile(filename, \"r\") as archive:\n",
    "                for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting files\", unit=\"files\", unit_scale=False):\n",
    "                    archive.extract(member, os.getcwd())\n",
    "        else:\n",
    "            if os.path.isfile(filename):\n",
    "                shutil.rmtree(\"Annotations\", ignore_errors=True) # clear out old data\n",
    "                with zipfile.ZipFile(filename, \"r\") as archive:\n",
    "                    for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting files\", unit=\"files\", unit_scale=False):\n",
    "                        archive.extract(member, os.getcwd())\n",
    "        shutil.move(\"Annotations/Annotations.mat\", \".\")\n",
    "        shutil.rmtree(\"Annotations\", ignore_errors=True)\n",
    "        shutil.rmtree(\"__MACOSX\", ignore_errors=True)\n",
    "        with contextlib.suppress(FileNotFoundError):\n",
    "            os.remove(filename)\n",
    "    mat_data = scipy.io.loadmat(\"Annotations.mat\")\n",
    "    return mat_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afead34f4f8649b7ba5a2d24d540d1c6",
      "74b539de4f754454a5abd3c2898a5987",
      "f0579c5d9da0478eb6b378deb2a3f493",
      "788d0530b65e4c34b1884011695ac223",
      "65d6148eb42544ff82e7c08ca3773be9",
      "4b7ae31de2224e8e8858bd8bc2330dd1",
      "84b572976f3f4505bd331e9fce5a7492",
      "301bc7352cb640ff846266581d20b8c9",
      "c9968884e90147f5aee2b0a6370871a4",
      "c235e630705540cebe376b469f7fd8b2",
      "c219219b78a04360ac845cec666abbe5"
     ]
    },
    "id": "AebeCHJudPEK",
    "outputId": "f7780790-1316-4be1-fb62-9afd3ae0dd2f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69941e71877d43a28a16250232a275f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Extracting files'), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mat_data = prepare_annotations(\"Annotations.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9ULSZlB-MW1y"
   },
   "outputs": [],
   "source": [
    "def set_imsize(image_size):\n",
    "    im_size = []\n",
    "    image_size = np.array(image_size).flatten().tolist()[0]\n",
    "    row = np.array(image_size[0]).flatten().tolist()[0]\n",
    "    col = np.array(image_size[1]).flatten().tolist()[0]\n",
    "    im_size.append(row)\n",
    "    im_size.append(col)\n",
    "    return im_size\n",
    "\n",
    "def validate_bbox(bbox, im_size):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1 = min(im_size[0], max(0, x1))\n",
    "        x2 = min(im_size[0], max(0, x2))\n",
    "        y1 = min(im_size[1], max(0, y1))\n",
    "        y2 = min(im_size[1], max(0, y2))\n",
    "        return [int(x1), int(y1), int(x2), int(y2)]\n",
    "\n",
    "def set_bbox(person_bbox, im_size):\n",
    "    return validate_bbox(np.array(person_bbox).flatten().tolist(), im_size)\n",
    "\n",
    "def set_emotions(person_emotions):\n",
    "    cat = np.array(person_emotions).flatten().tolist()\n",
    "    if not cat:\n",
    "        return\n",
    "    cat = np.array(cat[0]).flatten().tolist()\n",
    "    return [np.array(c).flatten().tolist()[0] for c in cat]\n",
    "\n",
    "def set_vad_scores(person_vad):\n",
    "    cont = np.array(person_vad).flatten().tolist()\n",
    "    if not cont:\n",
    "        return\n",
    "    return [np.array(c).flatten().tolist()[0] for c in cont[0]]\n",
    "\n",
    "def convert_annotations(mat_train):\n",
    "    rows = list()\n",
    "    for e in tqdm.tqdm(mat_train, desc=\"Converting annotations\", unit=\"files\", unit_scale=False):\n",
    "        for person in e[4][0]:\n",
    "            row = {\n",
    "                \"Filename\": e[0][0],\n",
    "                \"Folder\": e[1][0],\n",
    "                \"Gender\": person[3][0],\n",
    "                \"Age\": person[4][0]\n",
    "            }\n",
    "            row[\"Image Size\"] = set_imsize(e[2])\n",
    "            row[\"Bounding Box\"] = set_bbox(person[0], row[\"Image Size\"])\n",
    "            row[\"Emotions\"] = set_emotions(person[1])\n",
    "            row[\"VAD Scores\"] = set_vad_scores(person[2])\n",
    "            if pd.isnull(np.array(row[\"Emotions\"])).any():\n",
    "                row[\"Emotions\"] = None\n",
    "            if pd.isnull(np.array(row[\"VAD Scores\"])).any():\n",
    "                row[\"VAD Scores\"] = None\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(f\"annotations.csv\", index=False)\n",
    "    with contextlib.suppress(FileNotFoundError):\n",
    "        os.remove(\"Annotations.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d544938599eb40918e09063418e7be95",
      "bd51a136c3f4439ebcdd4f208fb8c6f3",
      "8257893c4d284d64a3884476b88d7bcc",
      "0c10210558ab422b8e8073619f0104a8",
      "2d0c67669d384f5dad8bcde7a9ed7134",
      "2814219b98c14bbd98f676f9eee02471",
      "3425a55c047d432e89e4a707f71507a8",
      "e42a7a566a1e4a8dbba3256bfc7a826d",
      "3d4e9d07cf9b45409d3bedb54cf0881f",
      "279df06c70624db6800fb1a4a8414fbb",
      "8cb96e114918446282e9c33d711b7e10"
     ]
    },
    "id": "kdiRpm18NndC",
    "outputId": "a19cceb8-c599-47d9-fbe8-49206230c43b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b1a4e9910643b3903906e500bbffaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Converting annotations'), FloatProgress(value=0.0, max=17077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_annotations(mat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiiKxcXykpiD"
   },
   "source": [
    "## Dataset loading, preprocessing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GnLGerzPh5BS"
   },
   "outputs": [],
   "source": [
    "class EmoticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, annotations_fname):\n",
    "        super().__init__()\n",
    "        self.dir = dir\n",
    "        self.annotations = pd.read_csv(annotations_fname)\n",
    "        self.annotations[\"Image Size\"] = self.annotations[\"Image Size\"].apply(eval)\n",
    "        self.annotations[\"Bounding Box\"] = self.annotations[\"Bounding Box\"].apply(eval)\n",
    "        self.annotations[\"Emotions\"] = self.annotations[\"Emotions\"].apply(eval)\n",
    "        self.annotations[\"VAD Scores\"] = self.annotations[\"VAD Scores\"].apply(eval)\n",
    "        self.mlb = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "        self.mlb.fit(self.annotations[\"Emotions\"].to_numpy())\n",
    "        self.context_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.PILToTensor(),\n",
    "            torchvision.transforms.ConvertImageDtype(torch.float32),\n",
    "            torchvision.transforms.Resize(CONTEXT_IMAGE_SIZE),\n",
    "            torchvision.transforms.Normalize(CONTEXT_NORM[\"mean\"], CONTEXT_NORM[\"std\"])\n",
    "        ])\n",
    "        self.body_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.PILToTensor(),\n",
    "            torchvision.transforms.ConvertImageDtype(torch.float32),\n",
    "            torchvision.transforms.Resize(BODY_IMAGE_SIZE),\n",
    "            torchvision.transforms.Normalize(BODY_NORM[\"mean\"], BODY_NORM[\"std\"])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.annotations.loc[index]\n",
    "        context = PIL.Image.open(pathlib.Path(self.dir) / row[\"Folder\"] / row[\"Filename\"]).convert(\"RGB\")\n",
    "        bbox = row[\"Bounding Box\"]\n",
    "        body = context.crop((bbox[0], bbox[1], bbox[2], bbox[3])).resize((BODY_IMAGE_SIZE, BODY_IMAGE_SIZE))\n",
    "        context = context.resize((CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE))\n",
    "        emotions = self.mlb.transform([row[\"Emotions\"]])\n",
    "        vad_scores = row[\"VAD Scores\"]\n",
    "        return self.context_transforms(context), self.body_transforms(body), torch.tensor(emotions).squeeze(0), torch.tensor(vad_scores) / 10.0\n",
    "\n",
    "    def get_emotion_name(self, output):\n",
    "        return self.mlb.inverse_transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikiBUuIY1re9",
    "outputId": "1953a0a0-1958-474c-8e11-739541e82a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n",
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=16V-PJBpdFakpzYTba3sdg_Fg3z9YED3C \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=16V-PJBpdFakpzYTba3sdg_Fg3z9YED3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F_wjAdfK7PCK"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(filename):\n",
    "    if not pathlib.Path(\"emotic\").is_dir():\n",
    "        if \"google.colab\" in str(IPython.get_ipython()):\n",
    "                with zipfile.ZipFile(filename, \"r\") as archive:\n",
    "                    for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting files\", unit=\"files\", unit_scale=False):\n",
    "                        archive.extract(member, os.getcwd())\n",
    "        else:\n",
    "            if os.path.isfile(filename):\n",
    "                shutil.rmtree(\"emotic\", ignore_errors=True) # clear out old data\n",
    "                with zipfile.ZipFile(filename, \"r\") as archive:\n",
    "                    for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting files\", unit=\"files\", unit_scale=False):\n",
    "                        archive.extract(member, os.getcwd())\n",
    "        # I hate MacOS\n",
    "        with contextlib.suppress(FileNotFoundError):\n",
    "            for path in pathlib.Path(\"emotic\").rglob(\"*.*\"):\n",
    "                if not str(path).endswith(\".jpg\"):\n",
    "                    os.remove(str(path))\n",
    "    if not pathlib.Path(\"annotations.csv\").is_file():\n",
    "        raise ValueError(\"No annotations file detected, cannot proceed\")\n",
    "    return EmoticDataset(\"emotic\", \"annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BmEWE4DrIur0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/content/drive/MyDrive/emotic.zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp /content/drive/MyDrive/emotic.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "19a27b0adedf49068771e098ca26306f",
      "8d49f8b9f5ab41cb8cdacaacbbe13421",
      "865e3c8361f24429b56bf3c50cdcfce6",
      "137b763f946f4825a1e2fe14038d0285",
      "c798e5ef9d254a73aece738860ab2f2a",
      "b0563ccc86c54912865cf08af538c604",
      "ab1982c56e1d4ea79c98611db8fdc2ac",
      "de10e772dd1245a38244d4037c23ea21",
      "ff2c87e82e4e44f995dd7360e5e11dc5",
      "43fb52512d4240a897cdbc46cc6a90ca",
      "6466ea348d7842fe8e53542b8eed14e2"
     ]
    },
    "id": "yrUMPqui8AeE",
    "outputId": "a11d0a3e-946d-41f4-8d3d-5302fbb89aa6"
   },
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(\"emotic.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3Pa6z40aFUFP"
   },
   "outputs": [],
   "source": [
    "trainset, remains = torch.utils.data.random_split(dataset, \n",
    "                                                  [int(0.7 * len(dataset)), len(dataset) - int(0.7 * len(dataset))],\n",
    "                                                  torch.Generator().manual_seed(RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YyX8xRGrJrWO"
   },
   "outputs": [],
   "source": [
    "valset, testset = torch.utils.data.random_split(remains, \n",
    "                                                [len(remains) // 2, len(remains) - len(remains) // 2],\n",
    "                                                torch.Generator().manual_seed(RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXEhaVl7L4h7"
   },
   "source": [
    "## Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-JqV_FBBJvCs"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, BATCH_SIZE, shuffle=True, num_workers=torch.multiprocessing.cpu_count(), pin_memory=True, drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, BATCH_SIZE, shuffle=False, num_workers=torch.multiprocessing.cpu_count(), pin_memory=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, BATCH_SIZE, shuffle=False, num_workers=torch.multiprocessing.cpu_count(), pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gHq3XcTPKan"
   },
   "source": [
    "## Selecting optimal \"tail\" CNN (based on inference time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pk8dKOGEMIr9"
   },
   "outputs": [],
   "source": [
    "mobilenet = torchvision.models.mobilenet.mobilenet_v3_small(pretrained=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e3x8Ev1_MIqf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV3                                        --                        --\n",
       "├─Sequential: 1-1                                  [32, 576, 7, 7]           --\n",
       "│    └─ConvNormActivation: 2-1                     [32, 16, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-1                            [32, 16, 112, 112]        432\n",
       "│    │    └─BatchNorm2d: 3-2                       [32, 16, 112, 112]        32\n",
       "│    │    └─Hardswish: 3-3                         [32, 16, 112, 112]        --\n",
       "│    └─InvertedResidual: 2-2                       [32, 16, 56, 56]          --\n",
       "│    │    └─Sequential: 3-4                        [32, 16, 56, 56]          744\n",
       "│    └─InvertedResidual: 2-3                       [32, 24, 28, 28]          --\n",
       "│    │    └─Sequential: 3-5                        [32, 24, 28, 28]          3,864\n",
       "│    └─InvertedResidual: 2-4                       [32, 24, 28, 28]          --\n",
       "│    │    └─Sequential: 3-6                        [32, 24, 28, 28]          5,416\n",
       "│    └─InvertedResidual: 2-5                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-7                        [32, 40, 14, 14]          13,736\n",
       "│    └─InvertedResidual: 2-6                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-8                        [32, 40, 14, 14]          57,264\n",
       "│    └─InvertedResidual: 2-7                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-9                        [32, 40, 14, 14]          57,264\n",
       "│    └─InvertedResidual: 2-8                       [32, 48, 14, 14]          --\n",
       "│    │    └─Sequential: 3-10                       [32, 48, 14, 14]          21,968\n",
       "│    └─InvertedResidual: 2-9                       [32, 48, 14, 14]          --\n",
       "│    │    └─Sequential: 3-11                       [32, 48, 14, 14]          29,800\n",
       "│    └─InvertedResidual: 2-10                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-12                       [32, 96, 7, 7]            91,848\n",
       "│    └─InvertedResidual: 2-11                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-13                       [32, 96, 7, 7]            294,096\n",
       "│    └─InvertedResidual: 2-12                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-14                       [32, 96, 7, 7]            294,096\n",
       "│    └─ConvNormActivation: 2-13                    [32, 576, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-15                           [32, 576, 7, 7]           55,296\n",
       "│    │    └─BatchNorm2d: 3-16                      [32, 576, 7, 7]           1,152\n",
       "│    │    └─Hardswish: 3-17                        [32, 576, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                           [32, 576, 1, 1]           --\n",
       "├─Sequential: 1-3                                  [32, 1000]                --\n",
       "│    └─Linear: 2-14                                [32, 1024]                590,848\n",
       "│    └─Hardswish: 2-15                             [32, 1024]                --\n",
       "│    └─Dropout: 2-16                               [32, 1024]                --\n",
       "│    └─Linear: 2-17                                [32, 1000]                1,025,000\n",
       "====================================================================================================\n",
       "Total params: 2,542,856\n",
       "Trainable params: 2,542,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.81\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 724.59\n",
       "Params size (MB): 10.17\n",
       "Estimated Total Size (MB): 754.03\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(mobilenet, (BATCH_SIZE, 3, CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nm265JTjM47F"
   },
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rX8FlF47Nq4Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   --                        --\n",
       "├─Conv2d: 1-1                            [32, 64, 112, 112]        9,408\n",
       "├─BatchNorm2d: 1-2                       [32, 64, 112, 112]        128\n",
       "├─ReLU: 1-3                              [32, 64, 112, 112]        --\n",
       "├─MaxPool2d: 1-4                         [32, 64, 56, 56]          --\n",
       "├─Sequential: 1-5                        [32, 64, 56, 56]          --\n",
       "│    └─BasicBlock: 2-1                   [32, 64, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-1                  [32, 64, 56, 56]          36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [32, 64, 56, 56]          128\n",
       "│    │    └─ReLU: 3-3                    [32, 64, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-4                  [32, 64, 56, 56]          36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [32, 64, 56, 56]          128\n",
       "│    │    └─ReLU: 3-6                    [32, 64, 56, 56]          --\n",
       "│    └─BasicBlock: 2-2                   [32, 64, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-7                  [32, 64, 56, 56]          36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [32, 64, 56, 56]          128\n",
       "│    │    └─ReLU: 3-9                    [32, 64, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-10                 [32, 64, 56, 56]          36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [32, 64, 56, 56]          128\n",
       "│    │    └─ReLU: 3-12                   [32, 64, 56, 56]          --\n",
       "├─Sequential: 1-6                        [32, 128, 28, 28]         --\n",
       "│    └─BasicBlock: 2-3                   [32, 128, 28, 28]         --\n",
       "│    │    └─Conv2d: 3-13                 [32, 128, 28, 28]         73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [32, 128, 28, 28]         256\n",
       "│    │    └─ReLU: 3-15                   [32, 128, 28, 28]         --\n",
       "│    │    └─Conv2d: 3-16                 [32, 128, 28, 28]         147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [32, 128, 28, 28]         256\n",
       "│    │    └─Sequential: 3-18             [32, 128, 28, 28]         8,448\n",
       "│    │    └─ReLU: 3-19                   [32, 128, 28, 28]         --\n",
       "│    └─BasicBlock: 2-4                   [32, 128, 28, 28]         --\n",
       "│    │    └─Conv2d: 3-20                 [32, 128, 28, 28]         147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [32, 128, 28, 28]         256\n",
       "│    │    └─ReLU: 3-22                   [32, 128, 28, 28]         --\n",
       "│    │    └─Conv2d: 3-23                 [32, 128, 28, 28]         147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [32, 128, 28, 28]         256\n",
       "│    │    └─ReLU: 3-25                   [32, 128, 28, 28]         --\n",
       "├─Sequential: 1-7                        [32, 256, 14, 14]         --\n",
       "│    └─BasicBlock: 2-5                   [32, 256, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-26                 [32, 256, 14, 14]         294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [32, 256, 14, 14]         512\n",
       "│    │    └─ReLU: 3-28                   [32, 256, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-29                 [32, 256, 14, 14]         589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [32, 256, 14, 14]         512\n",
       "│    │    └─Sequential: 3-31             [32, 256, 14, 14]         33,280\n",
       "│    │    └─ReLU: 3-32                   [32, 256, 14, 14]         --\n",
       "│    └─BasicBlock: 2-6                   [32, 256, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-33                 [32, 256, 14, 14]         589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [32, 256, 14, 14]         512\n",
       "│    │    └─ReLU: 3-35                   [32, 256, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-36                 [32, 256, 14, 14]         589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [32, 256, 14, 14]         512\n",
       "│    │    └─ReLU: 3-38                   [32, 256, 14, 14]         --\n",
       "├─Sequential: 1-8                        [32, 512, 7, 7]           --\n",
       "│    └─BasicBlock: 2-7                   [32, 512, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-39                 [32, 512, 7, 7]           1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [32, 512, 7, 7]           1,024\n",
       "│    │    └─ReLU: 3-41                   [32, 512, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-42                 [32, 512, 7, 7]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [32, 512, 7, 7]           1,024\n",
       "│    │    └─Sequential: 3-44             [32, 512, 7, 7]           132,096\n",
       "│    │    └─ReLU: 3-45                   [32, 512, 7, 7]           --\n",
       "│    └─BasicBlock: 2-8                   [32, 512, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-46                 [32, 512, 7, 7]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [32, 512, 7, 7]           1,024\n",
       "│    │    └─ReLU: 3-48                   [32, 512, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-49                 [32, 512, 7, 7]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [32, 512, 7, 7]           1,024\n",
       "│    │    └─ReLU: 3-51                   [32, 512, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [32, 512, 1, 1]           --\n",
       "├─Linear: 1-10                           [32, 1000]                513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 58.05\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 1271.92\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 1337.94\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(resnet18, (BATCH_SIZE, 3, CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hAO3t80xQelq"
   },
   "outputs": [],
   "source": [
    "#effnet = torchvision.models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.DEFAULT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WiuQIgsqkkpa"
   },
   "outputs": [],
   "source": [
    "#torchinfo.summary(effnet, (BATCH_SIZE, 3, CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1PRRr10zO_Qu"
   },
   "outputs": [],
   "source": [
    "def inference_time(model):\n",
    "    dummy_input = torch.randn(1, 3, CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE, dtype=torch.float).to(DEVICE)\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "    # GPU warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "    # Performance measuring\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            start.record()\n",
    "            _ = model(dummy_input)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = start.elapsed_time(end)\n",
    "            timings[rep] = curr_time\n",
    "\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    return mean_syn, std_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PEBMSUwbQF3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time for MobileNetV2 (with sync): 4.695280529658, stddev: 0.11411610695480455\n",
      "Mean inference time for ResNet18 (with sync): 5.653976531028747, stddev: 0.025489469143623225\n"
     ]
    }
   ],
   "source": [
    "mean_syn_mobnet, std_syn_mobnet = inference_time(mobilenet)\n",
    "print(f\"Mean inference time for MobileNetV2 (with sync): {mean_syn_mobnet}, stddev: {std_syn_mobnet}\")\n",
    "mean_syn_resnet, std_syn_resnet = inference_time(resnet18)\n",
    "print(f\"Mean inference time for ResNet18 (with sync): {mean_syn_resnet}, stddev: {std_syn_resnet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9Y1qi-rhiS"
   },
   "source": [
    "*Will check out ResNet performance in pre-deployment test, if the model turns out to be too large, will switch to MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe3WjHu1sJvY"
   },
   "source": [
    "## Model architecture definition and preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "U_pt5d_kXQmN"
   },
   "outputs": [],
   "source": [
    "def partialclass(cls, *args, **kwargs):\n",
    "    class NewCls(cls):\n",
    "        __init__ = functools.partialmethod(cls.__init__, *args, **kwargs)\n",
    "\n",
    "    return NewCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ayuFrYSZEmuT"
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, 'wb') as f:\n",
    "            for chunk in tqdm.tqdm(r.iter_content(chunk_size=8192), desc=\"Downloading\"):\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4bAmaUZervtt"
   },
   "outputs": [],
   "source": [
    "class EmoticNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._load_body_tail()\n",
    "        self._load_context_tail()\n",
    "        self.fusion_block = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"fusion_fc\", torch.nn.Linear(self.context_out_size+ self.body_out_size, 256)),\n",
    "            (\"fusion_batchnorm\", torch.nn.BatchNorm1d(256)),\n",
    "            (\"fusion_act\", torch.nn.ReLU(inplace=True)),\n",
    "            (\"fusion_dropout\", torch.nn.Dropout(p=0.5)),\n",
    "        ]))\n",
    "        self.emotion_dense = torch.nn.Linear(256, 26)\n",
    "        self.vad_dense = torch.nn.Linear(256, 3)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_ = True\n",
    "\n",
    "    def _load_context_tail(self):\n",
    "        pickle.load = functools.partial(pickle.load, encoding=\"latin1\")\n",
    "        unpickler_orig = pickle.Unpickler\n",
    "        pickle.Unpickler = partialclass(pickle.Unpickler, encoding=\"latin1\")\n",
    "        checkpoint_file = pathlib.Path(torch.hub.get_dir()) / \"checkpoints\" / \"resnet18_places365.pth.tar\"\n",
    "        if not checkpoint_file.is_file():\n",
    "            download_file(\"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\", checkpoint_file)\n",
    "        context_tail = torch.load(checkpoint_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "        torch.save(context_tail, pathlib.Path(torch.hub.get_dir()) / \"checkpoints\" / \"resnet.pth.tar\")\n",
    "        context_tail = torchvision.models.resnet18(num_classes=365)\n",
    "        checkpoint = torch.load(pathlib.Path(torch.hub.get_dir()) / \"checkpoints\" / \"resnet.pth.tar\", map_location=lambda storage, loc: storage)\n",
    "        state_dict = {str.replace(k, \"module.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
    "        context_tail.load_state_dict(state_dict)\n",
    "        context_tail.eval()\n",
    "        self.context_out_size = list(context_tail.children())[-1].in_features\n",
    "        context_tail = torch.nn.Sequential(*(list(context_tail.children())[:-1]))\n",
    "        context_tail.add_module(\"flatten\", torch.nn.Flatten())\n",
    "        self.context_tail = context_tail\n",
    "        pickle.load = pickle.load.func\n",
    "        pickle.Unpickler = unpickler_orig\n",
    "\n",
    "    def _load_body_tail(self):\n",
    "        body_tail = torchvision.models.resnet18(pretrained=True)\n",
    "        body_tail = body_tail.cpu()\n",
    "        self.body_out_size = list(body_tail.children())[-1].in_features\n",
    "        body_tail = torch.nn.Sequential(*(list(body_tail.children())[:-1]))\n",
    "        body_tail.add_module(\"flatten\", torch.nn.Flatten())\n",
    "        self.body_tail = body_tail\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        # required to move tails to the same device as the fusion block and heads\n",
    "        super()._apply(fn)\n",
    "        self.context_tail._apply(fn)\n",
    "        self.body_tail._apply(fn)\n",
    "        return self\n",
    "\n",
    "    def forward(self, context, body):\n",
    "        features_context = self.context_tail(context)\n",
    "        features_body = self.body_tail(body)\n",
    "        fused = torch.cat((features_context, features_body), 1)\n",
    "        fused_out = self.fusion_block(fused)\n",
    "        emotions = self.emotion_dense(fused_out)\n",
    "        vad = self.vad_dense(fused_out)\n",
    "        return emotions, vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_b85KrDZ9ZH_"
   },
   "outputs": [],
   "source": [
    "model = EmoticNet().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bGDRGrgTWo2G"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "EmoticNet                                     --                        --\n",
       "├─Sequential: 1-1                             [32, 512]                 --\n",
       "│    └─Conv2d: 2-1                            [32, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [32, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [32, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [32, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-5                        [32, 64, 56, 56]          --\n",
       "│    │    └─BasicBlock: 3-1                   [32, 64, 56, 56]          73,984\n",
       "│    │    └─BasicBlock: 3-2                   [32, 64, 56, 56]          73,984\n",
       "│    └─Sequential: 2-6                        [32, 128, 28, 28]         --\n",
       "│    │    └─BasicBlock: 3-3                   [32, 128, 28, 28]         230,144\n",
       "│    │    └─BasicBlock: 3-4                   [32, 128, 28, 28]         295,424\n",
       "│    └─Sequential: 2-7                        [32, 256, 14, 14]         --\n",
       "│    │    └─BasicBlock: 3-5                   [32, 256, 14, 14]         919,040\n",
       "│    │    └─BasicBlock: 3-6                   [32, 256, 14, 14]         1,180,672\n",
       "│    └─Sequential: 2-8                        [32, 512, 7, 7]           --\n",
       "│    │    └─BasicBlock: 3-7                   [32, 512, 7, 7]           3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [32, 512, 7, 7]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [32, 512, 1, 1]           --\n",
       "│    └─Flatten: 2-10                          [32, 512]                 --\n",
       "├─Sequential: 1-2                             [32, 512]                 --\n",
       "│    └─Conv2d: 2-11                           [32, 64, 64, 64]          9,408\n",
       "│    └─BatchNorm2d: 2-12                      [32, 64, 64, 64]          128\n",
       "│    └─ReLU: 2-13                             [32, 64, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-14                        [32, 64, 32, 32]          --\n",
       "│    └─Sequential: 2-15                       [32, 64, 32, 32]          --\n",
       "│    │    └─BasicBlock: 3-9                   [32, 64, 32, 32]          73,984\n",
       "│    │    └─BasicBlock: 3-10                  [32, 64, 32, 32]          73,984\n",
       "│    └─Sequential: 2-16                       [32, 128, 16, 16]         --\n",
       "│    │    └─BasicBlock: 3-11                  [32, 128, 16, 16]         230,144\n",
       "│    │    └─BasicBlock: 3-12                  [32, 128, 16, 16]         295,424\n",
       "│    └─Sequential: 2-17                       [32, 256, 8, 8]           --\n",
       "│    │    └─BasicBlock: 3-13                  [32, 256, 8, 8]           919,040\n",
       "│    │    └─BasicBlock: 3-14                  [32, 256, 8, 8]           1,180,672\n",
       "│    └─Sequential: 2-18                       [32, 512, 4, 4]           --\n",
       "│    │    └─BasicBlock: 3-15                  [32, 512, 4, 4]           3,673,088\n",
       "│    │    └─BasicBlock: 3-16                  [32, 512, 4, 4]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-19                [32, 512, 1, 1]           --\n",
       "│    └─Flatten: 2-20                          [32, 512]                 --\n",
       "├─Sequential: 1-3                             [32, 256]                 --\n",
       "│    └─Linear: 2-21                           [32, 256]                 262,400\n",
       "│    └─BatchNorm1d: 2-22                      [32, 256]                 512\n",
       "│    └─ReLU: 2-23                             [32, 256]                 --\n",
       "│    └─Dropout: 2-24                          [32, 256]                 --\n",
       "├─Linear: 1-4                                 [32, 26]                  6,682\n",
       "├─Linear: 1-5                                 [32, 3]                   771\n",
       "===============================================================================================\n",
       "Total params: 22,623,389\n",
       "Trainable params: 22,623,389\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 76.99\n",
       "===============================================================================================\n",
       "Input size (MB): 25.56\n",
       "Forward/backward pass size (MB): 1687.04\n",
       "Params size (MB): 90.49\n",
       "Estimated Total Size (MB): 1803.09\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, ((BATCH_SIZE, 3, CONTEXT_IMAGE_SIZE, CONTEXT_IMAGE_SIZE), (BATCH_SIZE, 3, BODY_IMAGE_SIZE, BODY_IMAGE_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v6aSBYRq5W_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 22 13:05:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   70C    P0    N/A /  N/A |   1181MiB /  3020MiB |     40%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     10870      C   ...irtualenvs/dls/bin/python     1177MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT64MRafiXsM"
   },
   "source": [
    "## Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "vwuNOPjviW8s"
   },
   "outputs": [],
   "source": [
    "class EmoticLoss(torch.nn.Module):\n",
    "    def __init__(self, weight_type=\"mean\", margin=1, ratio=0.5, shift=1.2, smooth=1e-4, vad_loss_type=\"sl1\", device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.weight_type = weight_type\n",
    "        self.margin = margin\n",
    "        self.ratio = ratio\n",
    "        self.shift = shift\n",
    "        self.smooth = smooth\n",
    "        self.device = device\n",
    "        self.vad_loss_type = vad_loss_type\n",
    "        if self.weight_type == \"mean\":\n",
    "            self.weights = torch.ones((1, 26)) / 26.0\n",
    "            self.weights.to(self.device)\n",
    "        elif self.weight_type == \"static\":\n",
    "            self.weights = torch.FloatTensor([0.1435, 0.1870, 0.1692, 0.1165, 0.1949, 0.1204, 0.1728, 0.1372, 0.1620,\n",
    "                                              0.1540, 0.1987, 0.1057, 0.1482, 0.1192, 0.1590, 0.1929, 0.1158, 0.1907,\n",
    "                                              0.1345, 0.1307, 0.1665, 0.1698, 0.1797, 0.1657, 0.1520, 0.1537]).unsqueeze(0)\n",
    "            self.weights.to(self.device)\n",
    "\n",
    "    def _create_dynamic_weights(self, target):\n",
    "        target_stats = torch.sum(target, dim=0).float().unsqueeze(dim=0).cpu()\n",
    "        weights = torch.zeros((1, 26))\n",
    "        weights[target_stats != 0] = 1.0 / torch.log(target_stats[target_stats != 0].data + self.shift)\n",
    "        weights[target_stats == 0] = self.smooth\n",
    "        return weights\n",
    "\n",
    "    def _emotions_loss(self, pred, target):\n",
    "        if self.weight_type == \"dynamic\":\n",
    "            self.weights = self._create_dynamic_weights(target)\n",
    "        return (((pred - target) ** 2) * self.weights).sum()\n",
    "\n",
    "    def _vad_loss_sl1(self, pred, target):\n",
    "        mae = torch.abs(pred - target)\n",
    "        loss = 0.5 * (mae ** 2)\n",
    "        loss[(mae > self.margin)] = mae[(mae > self.margin)] - 0.5\n",
    "        return loss.sum()\n",
    "\n",
    "    def _vad_loss_l2(self, pred, target):\n",
    "        mae = torch.abs(pred - target)\n",
    "        loss = mae ** 2\n",
    "        loss[(mae < self.margin)] = 0.0\n",
    "        return loss.sum()\n",
    "\n",
    "    \n",
    "    def forward(self, pred_emotions, target_emotions, pred_vad, target_vad):\n",
    "        emotions_loss = self._emotions_loss(pred_emotions, target_emotions)\n",
    "        if self.vad_loss_type == \"sl1\":\n",
    "            vad_loss = self._vad_loss_sl1(pred_vad, target_vad)\n",
    "        else:\n",
    "            vad_loss = self._vad_loss_l2(pred_vad, target_vad)\n",
    "        return emotions_loss * self.ratio + vad_loss * (1 - self.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki11f8zgnSvw"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INVh8UDgjTDl"
   },
   "outputs": [],
   "source": [
    "def fit_epoch(model, loader, optimizer, criterion, epoch, epochs, device=DEVICE):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for context, body, emotions, vad_scores in tqdm.tqdm(loader, desc=f\"Training, epoch {epoch + 1} / {epochs}\", unit=\"batches\", unit_scale=False):\n",
    "        try:\n",
    "            context, body, emotions, vad_scores = context.to(device), body.to(device), emotions.to(device), vad_scores.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_emotions, pred_vad = model(context, body)\n",
    "            loss = criterion(pred_emotions, emotions, pred_vad * 10, vad_scores * 1)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "            pred_emotions, pred_vad = pred_emotions.cpu(), pred_vad.cpu()\n",
    "            del pred_emotions, pred_vad\n",
    "        finally:\n",
    "            context, body, emotions, vad_scores = context.cpu(), body.cpu(), emotions.cpu(), vad_scores.cpu()\n",
    "            del context, body, emotions, vad_scores\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_epoch(model, loader, criterion, epoch, epochs, device=DEVICE):\n",
    "    losses = []\n",
    "    scores = []\n",
    "    model.eval()\n",
    "    for context, body, emotions, vad_scores in tqdm.tqdm(loader, desc=f\"Validating, epoch {epoch + 1} / {epochs}\", unit=\"batches\", unit_scale=False):\n",
    "        try:\n",
    "            context, body, emotions, vad_scores = context.to(device), body.to(device), emotions.to(device), vad_scores.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_emotions, pred_vad = model(context, body)\n",
    "                loss = criterion(pred_emotions, emotions, pred_vad * 10, vad_scores * 10)\n",
    "            losses.append(loss.item())\n",
    "            pred_emotions, pred_vad = pred_emotions.cpu(), pred_vad.cpu()\n",
    "            scores.append([sklearn.metrics.jaccard_score(emotions.detach().numpy(), pred_emotions.detach().numpy()), \n",
    "                           sklearn.metrics.jaccard_score(vad_scores.detach().numpy(), pred_vad.detach().numpy())\n",
    "            ])\n",
    "            del pred_emotions, pred_vad\n",
    "        finally:\n",
    "            context, body, emotions, vad_scores = context.cpu(), body.cpu(), emotions.cpu(), vad_scores.cpu()\n",
    "            del context, body, emotions, vad_scores\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.mean(losses), np.mean(np.array(scores), axis=1)\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, criterion, epochs, scheduler=None, device=DEVICE, start_epoch=0):\n",
    "    history = []\n",
    "    start_time = time.time()\n",
    "    with tqdm.tqdm(desc=\"Epoch\", total=epochs, unit=\"epoch\", unit_scale=False) as pbar:\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            try:\n",
    "                train_loss = fit_epoch(model, train_loader, optimizer, criterion, epoch, epochs, device)\n",
    "                val_loss, scores = eval_epoch(model, val_loader, optimizer, criterion, epoch, epochs, device)\n",
    "                IPython.display.clear_output(wait=True)\n",
    "\n",
    "                history.append((train_loss, val_loss, scores, optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                show_train_pics(model, epoch, history[-1])\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.refresh()\n",
    "\n",
    "            except KeyboardInterrupt as stop:\n",
    "                tqdm.tqdm.write(f\"Training interrupted at epoch {epoch + 1}, returning history\")\n",
    "                return history\n",
    "\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    tqdm.tqdm.write(f\"Training time: {train_time: 0.1f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def denormalize_image(image, norm_params):\n",
    "    return image * norm_params[\"std\"][0] + norm_params[\"mean\"][0]\n",
    "\n",
    "def show_train_pics(model, val_loader, epoch, epoch_stats, sample_size=4):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    header = \"Validation results on epoch {ep:03d}\\n\\\n",
    "    Train loss: {t_loss: 0.4f}, validation loss: {v_loss: 0.4f}, Jaccard score: {j_score: 0.4f}\"\n",
    "    contexts, bodies, _, _ = next(iter(val_loader))\n",
    "    contexts, bodies = contexts[:sample_size], bodies[:sample_size]\n",
    "    with torch.no_grad():\n",
    "        emotions, vad_scores = model(contexts, bodies)\n",
    "        emotions = dataset.get_emotion_name(emotions.gt(torch.full_like(emotions, 0.5)))\n",
    "        vad_scores *= 10\n",
    "    for i in range(sample_size):\n",
    "        plt.subplot(sample_size, 1, i + 1)\n",
    "        plt.imshow(np.clip(denormalize_image(bodies[i], BODY_NORM).permute(1, 2, 0).numpy(), 0, 1))\n",
    "        plt.title(f\"Emotions: {emotions[i]}, VAD scores: {vad_scores[i]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(header.format(ep=epoch + 1, t_loss=epoch_stats[0], v_loss=epoch_stats[1], j_score=epoch_stats[2]))\n",
    "    plt.show()\n",
    "    contexts, bodies, emotions, vad_scores = contexts.cpu(), bodies.cpu(), emotions.cpu(), vad_scores.cpu()\n",
    "    del contexts, bodies, emotions, vad_scores\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAcSSW0hzd44"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  loss, val_loss, _, _ = zip(*history)\n",
    "  plt.figure(figsize=(15, 9))\n",
    "  plt.plot(loss, label=\"Train loss\")\n",
    "  plt.plot(val_loss, label=\"Validation loss\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "  _, _, val_metric, _ = zip(*history)\n",
    "  plt.figure(figsize=(15, 9))\n",
    "  plt.plot(val_metric, label=\"Jaccard score\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.show()\n",
    "\n",
    "def plot_learn_rate(history):\n",
    "  _, _, _, learn_rate = zip(*history)\n",
    "  plt.figure(figsize=(15, 9))\n",
    "  plt.plot(learn_rate, label=\"Learn rate\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Learn rate\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPYQLDHA46it"
   },
   "outputs": [],
   "source": [
    "def load_model(path, model_arch, optim_class, optim_kwargs):\n",
    "  model = model_arch()\n",
    "  checkpoint = torch.load(path)\n",
    "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "  optim = optim_class(model.parameters(), **optim_kwargs)\n",
    "  optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "  epoch = checkpoint[\"epoch\"]\n",
    "  loss = checkpoint[\"loss\"]\n",
    "  history = checkpoint[\"history\"]\n",
    "  model.eval()\n",
    "  return model, optim, epoch, loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjP-aTUG48iX"
   },
   "outputs": [],
   "source": [
    "def save_history(path, history):\n",
    "  with open(path, \"wb\") as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV3JaB8149pq"
   },
   "outputs": [],
   "source": [
    "def load_history(path):\n",
    "  with open(path, \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "  \n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mltEnCzf5P7I"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYvl_kqe5Td4"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = EmoticLoss(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jx3mGd_6UHD"
   },
   "outputs": [],
   "source": [
    "history = fit(model, trainloader, valloader, optimizer, criterion, 50, scheduler=scheduler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XIld4tp6orS"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GwnCfLb6u1o"
   },
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZSX7JSo6xNK"
   },
   "outputs": [],
   "source": [
    "plot_learn_rate(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA_7WM1C63r2"
   },
   "source": [
    "*Will write model scoring later*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "emotion_recognition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c10210558ab422b8e8073619f0104a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_279df06c70624db6800fb1a4a8414fbb",
      "placeholder": "​",
      "style": "IPY_MODEL_8cb96e114918446282e9c33d711b7e10",
      "value": " 17077/17077 [00:03&lt;00:00, 4828.95files/s]"
     }
    },
    "137b763f946f4825a1e2fe14038d0285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43fb52512d4240a897cdbc46cc6a90ca",
      "placeholder": "​",
      "style": "IPY_MODEL_6466ea348d7842fe8e53542b8eed14e2",
      "value": " 12013/23198 [00:25&lt;00:06, 1805.02files/s]"
     }
    },
    "19a27b0adedf49068771e098ca26306f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d49f8b9f5ab41cb8cdacaacbbe13421",
       "IPY_MODEL_865e3c8361f24429b56bf3c50cdcfce6",
       "IPY_MODEL_137b763f946f4825a1e2fe14038d0285"
      ],
      "layout": "IPY_MODEL_c798e5ef9d254a73aece738860ab2f2a"
     }
    },
    "279df06c70624db6800fb1a4a8414fbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2814219b98c14bbd98f676f9eee02471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d0c67669d384f5dad8bcde7a9ed7134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "301bc7352cb640ff846266581d20b8c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3425a55c047d432e89e4a707f71507a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d4e9d07cf9b45409d3bedb54cf0881f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43fb52512d4240a897cdbc46cc6a90ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b7ae31de2224e8e8858bd8bc2330dd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6466ea348d7842fe8e53542b8eed14e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d6148eb42544ff82e7c08ca3773be9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b539de4f754454a5abd3c2898a5987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b7ae31de2224e8e8858bd8bc2330dd1",
      "placeholder": "​",
      "style": "IPY_MODEL_84b572976f3f4505bd331e9fce5a7492",
      "value": "Extracting files: 100%"
     }
    },
    "788d0530b65e4c34b1884011695ac223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c235e630705540cebe376b469f7fd8b2",
      "placeholder": "​",
      "style": "IPY_MODEL_c219219b78a04360ac845cec666abbe5",
      "value": " 19/19 [00:00&lt;00:00, 194.66files/s]"
     }
    },
    "8257893c4d284d64a3884476b88d7bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e42a7a566a1e4a8dbba3256bfc7a826d",
      "max": 17077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d4e9d07cf9b45409d3bedb54cf0881f",
      "value": 17077
     }
    },
    "84b572976f3f4505bd331e9fce5a7492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "865e3c8361f24429b56bf3c50cdcfce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de10e772dd1245a38244d4037c23ea21",
      "max": 23198,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff2c87e82e4e44f995dd7360e5e11dc5",
      "value": 12013
     }
    },
    "8cb96e114918446282e9c33d711b7e10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d49f8b9f5ab41cb8cdacaacbbe13421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0563ccc86c54912865cf08af538c604",
      "placeholder": "​",
      "style": "IPY_MODEL_ab1982c56e1d4ea79c98611db8fdc2ac",
      "value": "Extracting files:  52%"
     }
    },
    "ab1982c56e1d4ea79c98611db8fdc2ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afead34f4f8649b7ba5a2d24d540d1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74b539de4f754454a5abd3c2898a5987",
       "IPY_MODEL_f0579c5d9da0478eb6b378deb2a3f493",
       "IPY_MODEL_788d0530b65e4c34b1884011695ac223"
      ],
      "layout": "IPY_MODEL_65d6148eb42544ff82e7c08ca3773be9"
     }
    },
    "b0563ccc86c54912865cf08af538c604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd51a136c3f4439ebcdd4f208fb8c6f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2814219b98c14bbd98f676f9eee02471",
      "placeholder": "​",
      "style": "IPY_MODEL_3425a55c047d432e89e4a707f71507a8",
      "value": "Converting annotations: 100%"
     }
    },
    "c219219b78a04360ac845cec666abbe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c235e630705540cebe376b469f7fd8b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c798e5ef9d254a73aece738860ab2f2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9968884e90147f5aee2b0a6370871a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d544938599eb40918e09063418e7be95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd51a136c3f4439ebcdd4f208fb8c6f3",
       "IPY_MODEL_8257893c4d284d64a3884476b88d7bcc",
       "IPY_MODEL_0c10210558ab422b8e8073619f0104a8"
      ],
      "layout": "IPY_MODEL_2d0c67669d384f5dad8bcde7a9ed7134"
     }
    },
    "de10e772dd1245a38244d4037c23ea21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e42a7a566a1e4a8dbba3256bfc7a826d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0579c5d9da0478eb6b378deb2a3f493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_301bc7352cb640ff846266581d20b8c9",
      "max": 19,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9968884e90147f5aee2b0a6370871a4",
      "value": 19
     }
    },
    "ff2c87e82e4e44f995dd7360e5e11dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
